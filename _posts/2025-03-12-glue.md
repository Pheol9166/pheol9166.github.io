---
title:  "GLUE"
excerpt: "General Language Understatding Evaluation Benchmark 리뷰"

last_modified_at: 2025-03-12T08:06:00-05:00
---

![image](https://github.com/user-attachments/assets/80b9ba50-390d-4aa2-a38b-3eebe367b4b5)

## ✔ Benchmark?

벤치마크란 특정 태스크에 대한 성능을 평가하기 위한 데이터셋을 일컫습니다.  
다른 연구들과의 비교를 위한 기준이 되기도 합니다.

## ✔ GLUE?

GLUE(General Language Understanding Evaluation)는 다양한 NLU, 즉 언어 이해 능력을 평가하기 위한 성능지표입니다.  
일반적인 인간의 언어 이해 능력과 비교하기 위해 유사한 여러 태스크를 준비하여 평가합니다. QA, Sentiment Analysis, Textual Entailment 등이 포함됩니다.

### ✔ GLUE Tasks

GLUE에서는 9개의 태스크를 제공합니다.
![image](https://github.com/user-attachments/assets/a5095ec1-a745-4143-a7dc-1ea01a78d630)


- **CoLA**
  - The Corpus of Linguistic Acceptability로 언어 이론 관련 책이나 기사에서 가져온 데이터
  - 이진 분류 문제이고, 평가 지표는 Matthews correlation coefficient를 사용
  
     cf) Matthews correlation coefficient(Phi correlation coefficient)
    
      ![image](https://github.com/user-attachments/assets/71105637-14e3-4e4c-949b-e68986ceca95)  
       - -1과 1 사이의 값을 가지며, 1에 가까울수록 비슷함.

- **SST-2**
  - The Stanford Sentiment Treebank로 긍/부정 라벨링이 붙어 있는 데이터
  - 감성 분석으로 문장을 긍/부정으로 이진 분류함

- **MRPC**
  - The Microsoft Research Paraphrase Corpus로 문장 쌍과 그에 대한 라벨로 이루어짐
  - 문장 쌍이 의미론적으로 동일한지 판별하여 이진분류
  - 라벨 불균형으로 인해 F1-Score와 Accuracy 두 가지 평가지표 사용

- **QQP**
  - The Quora Question Pairs dataset으로 질문 페어로 구성됨
  - 두 개의 질문이 의미론적으로 같은지 판별
  - 라벨 불균형으로 인해 F1-Score와 Accuracy 두 가지 평가지표 사용
 
- **STS-B**
  - The Semantic Textual Similarity Benchmark로 뉴스 헤드라인, 이미지 및 비디오 캡셔 등에서 가져온 데이터
  - 인간이 문장 쌍에 대한 유사도를 1부터 5까지 라벨링하였고, 모델을 통해서 이 값을 예측함
  - Pearson 상관계수와 Spearman 상관계수를 지표로 사용

    cf) Pearson correlation coefficient  
     - 두 변수 사이 상관관계가 선형일 때 사용 가능.
     - 편차 = 평균과 예측값 간의 차이 + 예측값과 실제 값의 차이
       => 전체 편차 내에서 예측값과 평균 간의 차이가 차지하는 비율
  
        ![image](https://github.com/user-attachments/assets/e8d4a6ab-2068-41ea-a1d9-3d65964c4c34)  
        (x, y = 각 변수에 속하는 변량, n = 총 변량의 개수, z = 표준점수)
  
        x, y를 z점수로 변환하면 다음과 같음  
        ![image](https://github.com/user-attachments/assets/1398c502-b057-4483-926b-09337bf3032d)

       => 보통 제곱한 값인 결정계수를 많이 사용


- **MNLI**
  - The Multi-Genre Natural Language Corpus의 줄임말로 문장 쌍에 textual entailment이 라벨링 되어 있음
  - 전제(premise)와 가설(hypothesis) 쌍으로 문장 구성, 해당 문장 간의 관계를 세 가지로 예측  

    - entailment (가설이 전제를 함의를 수반함)
    - contradiction (가설과 전제가 모순)
    - neutral (entailment와 contradiction 모두 아닐 때)
      
      ![image](https://github.com/user-attachments/assets/2ff609e7-3382-449f-b18f-3db822484e78)

- **QNLI**
  - The Stanford Question Answering Dataset의 줄임말로 문단-질문 쌍으로 구성
  - 문단 중 한 문장이 질문에 대한 답을 갖고 있음
 
- **RTE**
  - The Recognizing Textual Entailment로 annual textual entailment challenge에서 데이터를 가져옴
  - netural과 contradiction을 모두 not-entailment 라벨로 지정하여 이진 분류로 만듬

- **WNLI**
  - Winograd NLI의 줄임말로, The Winograd Schema Challenge에서 데이터를 가져옴
  - 대명사가 포함된 문장을 읽고 그것이 가리키는 대상을 리스트에서 고름
